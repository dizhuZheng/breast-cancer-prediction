{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.5.0\n"
     ]
    }
   ],
   "source": [
    "import math, re, os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt   # matplotlib进行画图\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from kaggle_datasets import KaggleDatasets # Kaggle数据集\n",
    "import efficientnet.tfkeras as efn    # 导入efficientnet模型\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix \n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE # 可以让程序自动的选择最优的线程并行个数\n",
    "# 从TPU创建部署\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver() #如果先前设置好了ＴＰＵ＿ＮＡＭＥ环境变量，不需要再给参数．\n",
    "tf.config.experimental_connect_to_cluster(tpu) # 配置实验连接到群集\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu) # 初始化tpu系统\n",
    "strategy = tf.distribute.experimental.TPUStrategy(tpu) # 设置TPU部署\n",
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [512, 512]\n",
    "EPOCHS = 12\n",
    "GCS_PATH = GCS_DS_PATH + '/tfrecords-jpeg-512x512'\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\n",
    "VALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\n",
    "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec') \n",
    "CLASSES = ['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'wild geranium', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle', 'snapdragon', \"colt's foot\", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'cosmos', 'alpine sea holly', 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barberton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'daisy', 'common dandelion', 'petunia', 'wild pansy', 'primula', 'sunflower', 'lilac hibiscus', 'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia', 'cautleya spicata', 'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'iris', 'windflower', 'tree poppy', 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily', 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen ', 'watercress', 'canna lily', 'hippeastrum ', 'bee balm', 'pink quill', 'foxglove', 'bougainvillea', 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper', 'blackberry lily', 'common tulip', 'wild rose']\n",
    "\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['class'], tf.int32)\n",
    "    return image, label # returns a dataset of (image, label) pairs\n",
    "\n",
    "def read_unlabeled_tfrecord(example):\n",
    "    UNLABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n",
    "        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    idnum = example['id']\n",
    "    return image, idnum # returns a dataset of image(s)\n",
    "\n",
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False #disable order,increase speed\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(image, label, seed=2020):\n",
    "    # 以1比2的概率,输出image沿着第二维翻转的内容,即,width.否则按原样输出图像.\n",
    "    # 参数：\n",
    "    # image：形状为[height, width, channels]的三维张量.\n",
    "    # seed：一个Python整数,用于创建一个随机种子.查看tf.set_random_seed行为.\n",
    "    # 返回：一个与image具有相同类型和形状的三维张量.\n",
    "    image = tf.image.random_flip_left_right(image, seed=seed) \n",
    "#     image = tf.image.random_flip_up_down(image, seed=seed)\n",
    "#     image = tf.image.random_brightness(image, 0.1, seed=seed)\n",
    "#     image = tf.image.random_jpeg_quality(image, 85, 100, seed=seed)\n",
    "#     image = tf.image.resize(image, [530, 530])\n",
    "#     image = tf.image.random_crop(image, [512, 512], seed=seed)\n",
    "    #image = tf.image.random_saturation(image, 0, 2)\n",
    "    return image, label   \n",
    "\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "\n",
    "def get_training_dataset():\n",
    "    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n",
    "    # 将数据转换并行化\n",
    "    # 为num_parallel_calls 参数选择最佳值取决于您的硬件、训练数据的特征（例如其大小和形状）、Map 功能的成本以及在 CPU 上同时进行的其他处理；\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
    "    # 函数形式：repeat(count=None)\n",
    "    # 参数count:(可选）表示数据集应重复的次数。默认行为（如果count是None或-1）是无限期重复的数据集。\n",
    "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.shuffle(2048) #将数据打乱，括号中数值越大，混乱程度越大\n",
    "    dataset = dataset.batch(BATCH_SIZE) # 按照顺序将小批量中样本数目行数据合成一个小批量，最后一个小批量可能小于20\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(ordered=False):\n",
    "    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)# 按照顺序将小批量中样本数目行数据合成一个小批量，最后一个小批量可能小于20\n",
    "    dataset = dataset.cache() # 使用.cache()方法：当计算缓存空间足够时，将preprocess的数据存储在缓存空间中将大幅提高计算速度。\n",
    "    dataset = dataset.prefetch(AUTO)# pipeline（管道）读取数据，在训练时预取下一批（自动调整预取缓冲区大小）\n",
    "    return dataset\n",
    "\n",
    "def get_train_valid_datasets():\n",
    "    dataset = load_dataset(VALIDATION_FILENAMES+TRAINING_FILENAMES, labeled=True)\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
    "    # 函数形式：repeat(count=None)\n",
    "    # 参数count:(可选）表示数据集应重复的次数。默认行为（如果count是None或-1）是无限期重复的数据集。\n",
    "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.shuffle(2048) #将数据打乱，括号中数值越大，混乱程度越大\n",
    "    dataset = dataset.batch(BATCH_SIZE) # 按照顺序将小批量中样本数目行数据合成一个小批量，最后一个小批量可能小于20\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(ordered=False):\n",
    "    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    # the number of data items is written in the name of the .tfrec\n",
    "    # files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "NUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\n",
    "NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n",
    "print('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))\n",
    "\n",
    "ds_train = get_training_dataset()\n",
    "ds_valid = get_validation_dataset()\n",
    "ds_test = get_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_one_flower(image, title, subplot, red=False, titlesize=16):\n",
    "    plt.subplot(*subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    if len(title) > 0:\n",
    "        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
    "    return (subplot[0], subplot[1], subplot[2]+1)\n",
    "\n",
    "def display_training_curves(training, validation, title, subplot):\n",
    "    if subplot%10==1: # set up the subplots on the first call # 在第一次调用该函数时设置子图\n",
    "        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    ax = plt.subplot(subplot) #设置子图\n",
    "    ax.set_facecolor('#F8F8F8') #设置背景颜色\n",
    "    ax.plot(training) #画训练集的曲线\n",
    "    ax.plot(validation) #画测试集的曲线\n",
    "    ax.set_title('model '+ title)\n",
    "    ax.set_ylabel(title) #设置y轴标题\n",
    "    #ax.set_ylim(0.28,1.05)\n",
    "    ax.set_xlabel('epoch') #设置x轴标题\n",
    "    ax.legend(['train', 'valid.']) #设置图例\n",
    "\n",
    "def batch_to_numpy_images_and_labels(data):\n",
    "    images, labels = data \n",
    "    numpy_images = images.numpy() \n",
    "    numpy_labels = labels.numpy()\n",
    "    if numpy_labels.dtype == object: # 在这种情况下为二进制字符串，它们是图像ID字符串\n",
    "        numpy_labels = [None for _ in enumerate(numpy_images)]\n",
    "    # 如果没有标签，只有图像ID，则对标签返回None（测试数据就是这种情况）\n",
    "    return numpy_images, numpy_labels\n",
    "\n",
    "# 把实际类型和模型预测出来的模型一起显示在图片上方，对验证集预测完标签后和验证集的实际标签进行比较\n",
    "# label,图片中花朵的实际类别\n",
    "# correct_label，当前我们预测的类别\n",
    "def title_from_label_and_target(label, correct_label):\n",
    "    # 如果没有预测的类别，则返回实际类别，比如训练集\n",
    "    if correct_label is None:\n",
    "        return CLASSES[label], True\n",
    "    correct = (label == correct_label)\n",
    "    # 如果一致，则返回OK，不一致则返回NO加实际类别\n",
    "    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n",
    "                                CLASSES[correct_label] if not correct else ''), correct\n",
    "\n",
    "# 展示小批量图片，我们在下面的代码中经常展示20张照片\n",
    "def display_batch_of_images(databatch, predictions=None):\n",
    "    \"\"\"This will work with:\n",
    "    display_batch_of_images(images)   #只展示图片 测试集需要这个\n",
    "    display_batch_of_images(images, predictions) #展示图片加预测的类别 测试集需要这个\n",
    "    display_batch_of_images((images, labels)) #展示图片加实际标签 训练集需要这个\n",
    "    display_batch_of_images((images, labels), predictions) #展示图片+实际类别+预测类别 验证集需要这个\n",
    "    \"\"\"\n",
    "    images, labels = batch_to_numpy_images_and_labels(databatch)\n",
    "    if labels is None:\n",
    "        labels = [None for _ in enumerate(images)]\n",
    "        \n",
    "    # 自动平方：这将删除不适合正方形或矩形的数据\n",
    "    rows = int(math.sqrt(len(images)))\n",
    "    cols = len(images)//rows  #\" // \" 表示整数除法,返回不大于结果的一个最大的整数，向下取整\n",
    "    # 大小和间距\n",
    "    FIGSIZE = 13.0  #画图大小\n",
    "    SPACING = 0.1\n",
    "    subplot=(rows,cols,1)\n",
    "    if rows < cols:\n",
    "        # 如果行大于列\n",
    "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
    "    else:\n",
    "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
    "    # display\n",
    "    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n",
    "        title = '' if label is None else CLASSES[label]\n",
    "        correct = True\n",
    "        if predictions is not None:\n",
    "            title, correct = title_from_label_and_target(predictions[i], label)\n",
    "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # 经过测试可以在1x1到10x10图像上工作的魔术公式\n",
    "        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n",
    "    #layout\n",
    "    plt.tight_layout()\n",
    "    if label is None and predictions is None:\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    else:\n",
    "        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(cmat, score, precision, recall):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    ax = plt.gca()\n",
    "    ax.matshow(cmat, cmap='Reds')\n",
    "    ax.set_xticks(range(len(CLASSES)))\n",
    "    ax.set_xticklabels(CLASSES, fontdict={'fontsize': 7})\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
    "    ax.set_yticks(range(len(CLASSES)))\n",
    "    ax.set_yticklabels(CLASSES, fontdict={'fontsize': 7})\n",
    "    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    titlestring = \"\"\n",
    "    if score is not None:\n",
    "        titlestring += 'f1 = {:.3f} '.format(score)\n",
    "    if precision is not None:\n",
    "        titlestring += '\\nprecision = {:.3f} '.format(precision)\n",
    "    if recall is not None:\n",
    "        titlestring += '\\nrecall = {:.3f} '.format(recall)\n",
    "    if len(titlestring) > 0:\n",
    "        ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=15, linewidth=80)\n",
    "print(\"Training data shapes:\")\n",
    "for image, label in ds_train.take(3):\n",
    "    print(image.numpy().shape, label.numpy().shape)\n",
    "print(\"Training data label examples:\", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test data shapes:\")\n",
    "for image, idnum in ds_test.take(3):\n",
    "    print(image.numpy().shape, idnum.numpy().shape)\n",
    "print(\"Test data IDs:\", idnum.numpy().astype('U')) # U=unicode string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_iter = iter(ds_train.unbatch().batch(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    pretrained_model = tf.keras.applications.VGG16(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        include_shape=[*IMAGE_SIZE, 3]\n",
    "    )\n",
    "    pretrained_model.trainable = False \n",
    "    model = tf.keras.Sequential([\n",
    "        pretrained_model,\n",
    "        layers.GlobalAvgPool2D(),\n",
    "        # activation='softmax'：表示这个层将返回图片在104个类别上的概率，其中最大的概率表示这个图片的预测类别\n",
    "        # softmax激活函数的本质就是将一个K维的任意实数向量压缩（映射）成另一个K维的实数向量，其中向量中的每个元素取值都介于（0，1）之间并且和为1。\n",
    "        # 在多分类单标签问题中，可以用softmax作为最后的激活层，取概率最高的作为结果\n",
    "        tf.keras.layers.Dense(len(CLASSES), activation='softmax'),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy'],\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('p3': virtualenvwrapper)",
   "name": "python390jvsc74a57bd00c13263204c156fc1b9a6b5ab587a9cb9cc72936a00a2d253bc14f7b6d36feab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}